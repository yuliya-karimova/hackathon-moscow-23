{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from tqdm import notebook\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import torch\n",
    "import transformers as ppb\n",
    "import lightgbm as lgb\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import wordnet\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('text.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Капитальный ремонт нежилых помещений (склада)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Капитальный ремонт нежилых помещений (склада)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Капитальный ремонт нежилых помещений (склада)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Капитальный ремонт нежилых помещений (склада)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Капитальный ремонт нежилых помещений (склада)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8842</th>\n",
       "      <td>2</td>\n",
       "      <td>Капитальный ремонт гидроизоляции фундаментов и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8843</th>\n",
       "      <td>2</td>\n",
       "      <td>Капитальный ремонт гидроизоляции фундаментов и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8844</th>\n",
       "      <td>2</td>\n",
       "      <td>Капитальный ремонт гидроизоляции фундаментов и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845</th>\n",
       "      <td>2</td>\n",
       "      <td>Капитальный ремонт гидроизоляции фундаментов и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8846</th>\n",
       "      <td>2</td>\n",
       "      <td>Капитальный ремонт гидроизоляции фундаментов и...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8847 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "0            1      Капитальный ремонт нежилых помещений (склада)\n",
       "1            1      Капитальный ремонт нежилых помещений (склада)\n",
       "2            1      Капитальный ремонт нежилых помещений (склада)\n",
       "3            1      Капитальный ремонт нежилых помещений (склада)\n",
       "4            1      Капитальный ремонт нежилых помещений (склада)\n",
       "...        ...                                                ...\n",
       "8842         2  Капитальный ремонт гидроизоляции фундаментов и...\n",
       "8843         2  Капитальный ремонт гидроизоляции фундаментов и...\n",
       "8844         2  Капитальный ремонт гидроизоляции фундаментов и...\n",
       "8845         2  Капитальный ремонт гидроизоляции фундаментов и...\n",
       "8846         2  Капитальный ремонт гидроизоляции фундаментов и...\n",
       "\n",
       "[8847 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters for category are: {'model': LogisticRegression(C=1, random_state=42, solver='liblinear'), 'model__C': 1, 'model__penalty': 'l2'}\n",
      "Best cross-validation score for category is: 1.0\n",
      "Test F1 score for category is: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Загрузите данные\n",
    "\n",
    "nltk_stopwords = list(nltk_stopwords.words('russian'))\n",
    "extra_stopwords = [\n",
    "    # (Ваш список стоп-слов)\n",
    "]\n",
    "nltk_stopwords.extend(extra_stopwords)\n",
    "count_tf_idf = TfidfVectorizer(stop_words=nltk_stopwords)\n",
    "\n",
    "def train_model(target_col):\n",
    "    \n",
    "    # Извлекаем признаки и целевую переменную\n",
    "    X = data['text']\n",
    "    y = data[target_col].values\n",
    "\n",
    "    # Разделите данные на тренировочные и тестовые наборы\n",
    "\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    except ValueError:\n",
    "        # В случае ошибки отключаем стратификацию\n",
    "        print(f\"Stratification failed for {target_col}. Proceeding without stratification.\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Примените TF-IDF векторизатор к вашим данным\n",
    "    tfidf_train = count_tf_idf.fit_transform(X_train)\n",
    "    tfidf_test = count_tf_idf.transform(X_test)\n",
    "\n",
    "    # Определите конвейер и сетку параметров для поиска по сетке\n",
    "    pipe = Pipeline([\n",
    "        ('model', LogisticRegression(random_state=1, solver='liblinear', max_iter=200))\n",
    "    ])\n",
    "\n",
    "    param_grid = [\n",
    "        {\n",
    "            'model': [LogisticRegression(random_state=42, solver='liblinear')],\n",
    "            'model__C': list(range(1, 15, 3)),\n",
    "            'model__penalty': ['l1', 'l2']\n",
    "        },\n",
    "        {\n",
    "            'model': [SVC(random_state=42)],\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Проведите поиск по сетке, чтобы найти наилучшие параметры\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, scoring=make_scorer(f1_score, average='weighted'), cv=5, verbose=True, n_jobs=-1)\n",
    "    best_grid = grid.fit(tfidf_train, y_train)\n",
    "    \n",
    "    # Сохраните модель\n",
    "    joblib.dump(best_grid, f\"{target_col}_model.joblib\")\n",
    "\n",
    "    # Выведите наилучшие параметры и оценку\n",
    "    print(f\"Best parameters for {target_col} are:\", grid.best_params_)\n",
    "    print(f\"Best cross-validation score for {target_col} is:\", grid.best_score_)\n",
    "\n",
    "    # Проверка на тестовой выборке\n",
    "    test_predictions = best_grid.predict(tfidf_test)\n",
    "    test_score = f1_score(y_test, test_predictions, average='weighted')\n",
    "    print(f\"Test F1 score for {target_col} is:\", test_score)\n",
    "    \n",
    "    return grid.best_score_, test_score\n",
    "\n",
    "# Создание словаря для хранения обученных моделей\n",
    "models = {}\n",
    "\n",
    "# Обучите модели для каждого агентства\n",
    " \n",
    "models = train_model('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('text.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = data.drop_duplicates(subset='text', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Хотите BERT вместо distilBERT? Раскомментируйте следующую строку:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "tokenized = data['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=200,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0d10eb5e714120b97fc229aa8560cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.tensor(np.array(padded[batch_size*i:batch_size*(i+1)])) \n",
    "        attention_mask = torch.tensor(np.where(batch != 0, 1, 0))\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "        np.save('example_1', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100, 29]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\KarimovDO\\Documents\\GitHub\\hak5\\text.ipynb Ячейка 11\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KarimovDO/Documents/GitHub/hak5/text.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mexample_1.npy\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KarimovDO/Documents/GitHub/hak5/text.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m target \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/KarimovDO/Documents/GitHub/hak5/text.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_features, test_features, train_target, test_target \u001b[39m=\u001b[39m train_test_split(features, target)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KarimovDO/Documents/GitHub/hak5/text.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_features, features_vr, train_target, target_vr \u001b[39m=\u001b[39m train_test_split(features, target, test_size \u001b[39m=\u001b[39m \u001b[39m0.4\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KarimovDO/Documents/GitHub/hak5/text.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m valid_features, test_features, valid_target, test_target \u001b[39m=\u001b[39m train_test_split(features_vr, target_vr, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2417\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2420\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   2422\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 378\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    379\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 29]"
     ]
    }
   ],
   "source": [
    "features = np.concatenate(np.load('example_1.npy'))\n",
    "target = data['category']\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, target)\n",
    "train_features, features_vr, train_target, target_vr = train_test_split(features, target, test_size = 0.4, random_state = 42)\n",
    "valid_features, test_features, valid_target, test_target = train_test_split(features_vr, target_vr, test_size=0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling(LogisticRegression(random_state=12345, C = 4, penalty = 'l1', solver='liblinear', max_iter=200),\n",
    "         train_features, train_target, valid_features, valid_target) \n",
    "modeling(lgb.LGBMClassifier(), train_features, train_target, valid_features, valid_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
